{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646d3387",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "646d3387",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e9dfeecf134f0d6e60fabbca6650f6e",
     "grade": false,
     "grade_id": "cell-afeaf2ba2853c58b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from collections import deque\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from utils import save_model, load_model, get_augmented_data\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e8344",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "973e8344",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e41ac5a8880d05ee46863dd2b4c6e61a",
     "grade": false,
     "grade_id": "cell-90c7dca35f25c7fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## VAE Model\n",
    "\n",
    "Complete the conditional VAE model with structure shown in doc strings.\n",
    "\n",
    "**Hint**: we usually output logarithm standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f8e820",
   "metadata": {
    "deletable": false,
    "id": "a4f8e820",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "447c2a16f2258778ef4fb4dfed6ea2a4",
     "grade": false,
     "grade_id": "cell-ac7b8111ec3c40f1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, img_size, label_size, latent_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.img_size = img_size  # (C, H, W)\n",
    "        self.label_size = label_size\n",
    "        self.latent_size = latent_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.flat_img_size = img_size[0] * img_size[1] * img_size[2]\n",
    "        self.fc_img_enc = nn.Linear(self.flat_img_size, self.hidden_size)\n",
    "        self.fc_lbl_enc = nn.Linear(self.label_size, self.hidden_size)\n",
    "        self.encoder = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.encoder2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.fc_mu = nn.Linear(self.hidden_size, self.latent_size)\n",
    "        self.fc_logstd = nn.Linear(self.hidden_size, self.latent_size)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = x.view(x.size(0), -1).float()\n",
    "        # if y.shape[-1] != self.label_size:\n",
    "        #     y = F.one_hot(y, num_classes=self.label_size).float()\n",
    "        y = y.float()\n",
    "        x = F.relu(self.fc_img_enc(x))\n",
    "        y = F.relu(self.fc_lbl_enc(y))\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        x = F.relu(self.encoder(x))\n",
    "        x = F.relu(self.encoder2(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        logstd = self.fc_logstd(x)\n",
    "        return mu, logstd\n",
    "\n",
    "    def reparametrize(self, mu: torch.Tensor, logstd: torch.Tensor):\n",
    "        std_dev = torch.exp(logstd * 0.5)\n",
    "        eps = torch.randn_like(std_dev)\n",
    "        z = mu + eps * std_dev\n",
    "        return z\n",
    "\n",
    "    def encode(self, x, y):\n",
    "        mu, logstd = self.forward(x, y)\n",
    "        z = self.reparametrize(mu, logstd)\n",
    "        return z, mu, logstd\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, img_size, label_size, latent_size, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.img_size = img_size  # (C, H, W)\n",
    "        self.label_size = label_size\n",
    "        self.latent_size = latent_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.fc_latent = nn.Linear(self.latent_size, self.hidden_size)\n",
    "        self.fc_lbl_dec = nn.Linear(self.label_size, self.hidden_size)\n",
    "        self.decoder = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.decoder2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.fc_dec = nn.Linear(self.hidden_size, self.flat_img_size)\n",
    "\n",
    "    @property\n",
    "    def flat_img_size(self):\n",
    "        return self.img_size[0] * self.img_size[1] * self.img_size[2]\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        # if y.shape[-1] != self.label_size:\n",
    "        #     y = F.one_hot(y, num_classes=self.label_size).float()\n",
    "        y = y.float()\n",
    "        z = F.relu(self.fc_latent(z))\n",
    "        y = F.relu(self.fc_lbl_dec(y))\n",
    "        z = torch.cat((z, y), dim=1)\n",
    "        z = F.relu(self.decoder(z))\n",
    "        z = F.relu(self.decoder2(z))\n",
    "        x = self.fc_dec(z)\n",
    "        x = x.view(x.size(0), *self.img_size)\n",
    "        x = torch.sigmoid(x)  # Apply sigmoid activation to get pixel values in [0, 1]\n",
    "        return x\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, img_size, label_size, latent_size, hidden_size):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.img_size = img_size  # (C, H, W)\n",
    "        self.label_size = label_size\n",
    "        self.latent_size = latent_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = Encoder(img_size, label_size, latent_size, hidden_size)\n",
    "        self.decoder = Decoder(img_size, label_size, latent_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        z, mu, logstd = self.encoder.encode(x, y)\n",
    "        x_recon = self.decoder(z, y)\n",
    "        return x_recon, mu, logstd\n",
    "    \n",
    "    def encode_param(self, x, y):\n",
    "        # compute mu and logstd of p(z|x)\n",
    "        mu, logstd = self.encoder(x, y)\n",
    "        return mu, logstd\n",
    "    \n",
    "    def  reparamaterize(self, mu: torch.Tensor, logstd: torch.Tensor):\n",
    "        # compute latent z with reparameterization trick\n",
    "        std_dev = torch.exp(logstd*0.5)\n",
    "        eps = torch.randn_like(std_dev)\n",
    "        z = mu + eps * std_dev\n",
    "        return z\n",
    "    \n",
    "    def encode(self, x, y):\n",
    "        # sample latent z from p(z|x)\n",
    "        mu, logstd = self.encode_param(x, y)\n",
    "        z = self.reparamaterize(mu, logstd)\n",
    "        return z, mu, logstd\n",
    "    \n",
    "    def decode(self, z, y):\n",
    "        recon_x = self.decoder(z, y)\n",
    "        return recon_x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample_images(self, label, save=True, save_dir='./vae'):\n",
    "        self.eval()\n",
    "        n_samples = label.shape[0]\n",
    "        samples  = self.decoder.decode(torch.randn(n_samples, self.latent_size).to(label.device), label)\n",
    "        imgs = samples.view(n_samples, 1, 28, 28).clamp(0., 1.)\n",
    "        if save:\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            torchvision.utils.save_image(imgs, os.path.join(save_dir, 'sample.png'), nrow=int(np.sqrt(n_samples)))\n",
    "        return imgs\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9806dfb4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9806dfb4",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59258bed89bac2ba13c2f27ebcfc60f7",
     "grade": false,
     "grade_id": "cell-bff0aa54c09237bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## VAE Loss\n",
    "\n",
    "Given image $x$ and corresponding label $y$, compute the VAE loss in the following function.\n",
    "\n",
    "**Hint**: $p(x|z, y)$ is a real-valued Gaussian distribution, while images are in range $[0, 1]$. Therefore, you may want to transform $x$ when computing $p(x|z, y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ba01a",
   "metadata": {
    "deletable": false,
    "id": "207ba01a",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08f2d7d9aef79e56361662c6740977da",
     "grade": false,
     "grade_id": "cell-cde6ea3519fbcd6e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_vae_loss(vae_model, x, y_en, y_de, beta=1):\n",
    "    # compute vae loss for input x and label y\n",
    "    z, mu, logstd = vae_model.encode(x, y_en)\n",
    "    x_hat = vae_model.decode(z, y_de)\n",
    "    x_hat = x_hat.reshape(x.size(0), -1)\n",
    "    # compute reconstruction loss\n",
    "    recon_loss = torch.sum((x_hat - x.view(x.size(0), -1))**2, dim=1)\n",
    "    # compute KL divergence\n",
    "    kl_div = -0.5 * torch.sum(1 + logstd - mu.pow(2) - logstd.exp(), dim=-1)\n",
    "    # compute total loss\n",
    "    loss = recon_loss + beta * kl_div\n",
    "    return loss, recon_loss, beta * kl_div\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e81e1",
   "metadata": {
    "id": "a48e81e1"
   },
   "source": [
    "## Training & Evaluation\n",
    "\n",
    "We have implemented the training and evaluation functions. Feel free to modify `train` if you want to monitoring more information. Make sure your best model is stored in `'./vae/vae_best.pth'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca14731",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0ca14731",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3dd49cb59b62e08e8b074e6d62eaacbb",
     "grade": false,
     "grade_id": "cell-de07c12138584171",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(vae_model, loader, device, beta):\n",
    "    vae_model.eval()\n",
    "    val_loss = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    pbar = tqdm(total=len(loader.dataset))\n",
    "    pbar.set_description('Eval')\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        n_batches += x.shape[0]\n",
    "        x = x.view(x.shape[0], -1).to(device)\n",
    "        if y.shape[-1] ==4:\n",
    "            y = y[:, 0]\n",
    "        y = y.to(device)\n",
    "\n",
    "        # print(x.shape, y.shape)\n",
    "        loss, recon_loss, kl_div = compute_vae_loss(vae_model, x, y, beta)\n",
    "        val_loss += loss.sum().item()\n",
    "        pbar.update(x.size(0))\n",
    "        pbar.set_description('Val Loss: {:.6f}'.format(val_loss / n_batches))\n",
    "\n",
    "    pbar.close()\n",
    "    return val_loss / n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f7857a",
   "metadata": {
    "id": "86f7857a"
   },
   "outputs": [],
   "source": [
    "def train(n_epochs, vae_model, train_loader, val_loader, optimizer, beta=1, \n",
    "          device=torch.device('cuda'), save_interval=10, \n",
    "          en_drate = 0.5, de_drate = 0.5):\n",
    "    vae_model.to(device)\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0\n",
    "        tot_recon_loss = 0\n",
    "        tot_kl_div = 0\n",
    "        n_batches = 0\n",
    "        if epoch % 20 ==0:\n",
    "            pbar = tqdm(total=len(train_loader.dataset))\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            # compute loss\n",
    "            vae_model.train()\n",
    "            n_batches += x.shape[0]\n",
    "            x = x.view(x.shape[0], -1).to(device)\n",
    "            if y.shape[-1] ==4:\n",
    "                y = y[:, 0]\n",
    "            y = y.to(device)\n",
    "            y = F.one_hot(y.long(), num_classes=11)\n",
    "\n",
    "            bsz = x.shape[0]\n",
    "            # randomly drop labels\n",
    "\n",
    "            y_en = y.clone()\n",
    "            y_de = y.clone()\n",
    "            mask_en = torch.rand(bsz).to(device) < en_drate\n",
    "            mask_de = torch.rand(bsz).to(device) < de_drate\n",
    "            y_en[mask_en] = 0\n",
    "            y_de[mask_de] = 0\n",
    "            y_en[mask_en,-1] = 1\n",
    "            y_de[mask_de,-1] = 1\n",
    "\n",
    "\n",
    "            loss, recon_loss, kl_div = compute_vae_loss(vae_model, x, y_en, y_de, beta)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.sum().item()\n",
    "            tot_recon_loss += recon_loss.sum().item()\n",
    "            tot_kl_div += kl_div.sum().item()\n",
    "            if epoch % 20 ==0:\n",
    "                pbar.update(x.size(0))\n",
    "                pbar.set_description('Train Epoch {}, Train Loss: {:.6f}, recon_loss: {:.6f}, kl_div {:.6f}'.format(epoch + 1, train_loss / n_batches, \n",
    "                                                                                                                tot_recon_loss / n_batches, tot_kl_div / n_batches))\n",
    "        pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fc1dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, latend_dim, hidden_dim, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.latent_dim = latend_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.fc1 = nn.Linear(self.latent_dim, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.hidden_dim//4)\n",
    "        self.fc3 = nn.Linear(self.hidden_dim//4, self.num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c5310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, vae_model, train_loader, val_loader, optimizer, n_epochs=100, device=torch.device('cuda')):\n",
    "    model.to(device)\n",
    "    vae_model.to(device)\n",
    "    vae_model.eval()\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0\n",
    "        n_batches = 0\n",
    "        if epoch % 20 ==0:\n",
    "            pbar = tqdm(total=len(train_loader.dataset))\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            # compute loss\n",
    "            model.train()\n",
    "            n_batches += x.shape[0]\n",
    "            x = x.view(x.shape[0], -1).to(device)\n",
    "            if y.shape[-1] ==4:\n",
    "                y = y[:, 0]\n",
    "            y = y.to(device)\n",
    "            one_hot_y = F.one_hot(y.long(), num_classes=11).float()\n",
    "            # print()\n",
    "            with torch.no_grad():\n",
    "                z, _, _ = vae_model.encode(x, one_hot_y)\n",
    "            # z = z*11\n",
    "            pred = model(z)\n",
    "            # print(pred.shape, y.shape)\n",
    "\n",
    "            loss = F.cross_entropy(pred, y)\n",
    "            acc = (pred.argmax(dim=1) == y).float().mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if epoch % 20 ==0:\n",
    "                pbar.update(x.size(0))\n",
    "                pbar.set_description('Train Epoch {}, Train Loss: {:.6f}, Train acc{:.6f}, '.format(epoch + 1, train_loss / n_batches, acc.item()))\n",
    "                \n",
    "        pbar.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb3743e",
   "metadata": {
    "id": "adb3743e"
   },
   "source": [
    "## Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6e1c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dim = 11\n",
    "img_dim = (1, 28, 28)\n",
    "latent_dim = 10\n",
    "hidden_dim = 400\n",
    "\n",
    "def train_vae_with_drop_lbl_rate(train_loader, val_loader, en_drate, de_drate, num_epochs=100):\n",
    "    # define model\n",
    "\n",
    "    vae_model = CVAE(img_dim, label_dim, latent_dim, hidden_dim)\n",
    "    optimizer = torch.optim.Adam(vae_model.parameters(), lr=2e-4)\n",
    "\n",
    "    # train vae model\n",
    "    train(num_epochs, vae_model, train_loader, val_loader,\n",
    "          optimizer, beta=1, device=device, \n",
    "            en_drate=en_drate, de_drate=de_drate)\n",
    "    \n",
    "    return vae_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7629c75",
   "metadata": {
    "id": "d7629c75"
   },
   "outputs": [],
   "source": [
    "augmented_train_set , augmented_val_set = get_augmented_data()\n",
    "train_loader = DataLoader(augmented_train_set, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(augmented_val_set, batch_size=512, shuffle=False)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx}: data shape={data.shape}, target shape={target.shape}\")\n",
    "    # 在这里可以添加训练代码\n",
    "    break\n",
    "\n",
    "en_drate_schedule = torch.linspace(0.0, 1.0, 5)\n",
    "de_drate_schedule = torch.linspace(0.0, 1.0, 5)\n",
    "\n",
    "vae_models = []\n",
    "\n",
    "# for drop_lbl_rate in drop_lbl_rate_schedule:\n",
    "#     print(\"Trainning with Drop label rate: \", drop_lbl_rate)\n",
    "#     vae_model = train_vae_with_drop_lbl_rate(train_loader, val_loader, drop_lbl_rate, num_epochs=100)\n",
    "#     vae_models.append({\"drop_lbl_rate\": drop_lbl_rate, \"model\": vae_model})\n",
    "\n",
    "\n",
    "for en_drate in en_drate_schedule:\n",
    "    for de_drate in de_drate_schedule:\n",
    "        print(\"Training with Drop label rate: \", en_drate, de_drate)\n",
    "        vae_model = train_vae_with_drop_lbl_rate(train_loader, val_loader, en_drate, de_drate, num_epochs=250)\n",
    "        vae_models.append({\"en_drate\": en_drate, \"de_drate\": de_drate, \"model\": vae_model})\n",
    "        torch.save(vae_model.state_dict(), f\"./vae_ED/vae_{en_drate}_{de_drate}.pth\")\n",
    "        torch.save(vae_model.encoder.state_dict(), f\"./vae_ED/encoder_{en_drate}_{de_drate}.pth\")\n",
    "        torch.save(vae_model.decoder.state_dict(), f\"./vae_ED/decoder_{en_drate}_{de_drate}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95910115",
   "metadata": {},
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df221c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval classifier\n",
    "def eval_vae_classifier(classifier, vae_model, test_loader, device):\n",
    "    classifier.to(device)\n",
    "    vae_model.to(device)\n",
    "    classifier.eval()\n",
    "    vae_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.view(x.shape[0], -1).to(device)\n",
    "            y = y.to(device)\n",
    "            if y.shape[-1] ==4:\n",
    "                y = y[:, 0]\n",
    "            one_hot_y = F.one_hot(y.long(), num_classes=11).float()\n",
    "            z, _, _ = vae_model.encode(x, one_hot_y)\n",
    "            pred = classifier(z)\n",
    "            pred_label = pred.argmax(dim=1)\n",
    "            correct += (pred_label == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8faa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_classifier_pairs = []\n",
    "\n",
    "\n",
    "for i, vae_model_dict in enumerate(vae_models):\n",
    "    print(\" ebn_drate: \", vae_model_dict[\"en_drate\"], \"de_drate: \", vae_model_dict[\"de_drate\"])\n",
    "    vae_model = vae_model_dict[\"model\"]\n",
    "    # train a classifier\n",
    "    classifier = Classifier(latend_dim=vae_model.latent_size, hidden_dim=400, num_classes=11)\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=2e-4)\n",
    "    train_classifier(classifier, vae_model, train_loader, val_loader, optimizer, n_epochs=200)\n",
    "    # vae_classifier_pairs.append({\"drop_lbl_rate\": vae_model_dict[\"drop_lbl_rate\"], \"vae_model\": vae_model, \"classifier\": classifier})\n",
    "    vae_classifier_pairs.append({\"en_drate\": vae_model_dict[\"en_drate\"], \"de_drate\": vae_model_dict[\"de_drate\"], \"vae_model\": vae_model, \"classifier\": classifier})\n",
    "\n",
    "os.makedirs(\"vae_classifier_pairs\", exist_ok=True)\n",
    "\n",
    "for i, vae_classifier_pair in enumerate(vae_classifier_pairs):\n",
    "    en_drate = vae_classifier_pair[\"en_drate\"]\n",
    "    de_drate = vae_classifier_pair[\"de_drate\"]\n",
    "    vae_model = vae_classifier_pair[\"vae_model\"]\n",
    "    classifier = vae_classifier_pair[\"classifier\"]\n",
    "    torch.save(vae_model.state_dict(), os.path.join(\"vae_classifier_pairs\", \"vae_model_{}_{}.pth\".format(en_drate, de_drate)))\n",
    "    torch.save(classifier.state_dict(), os.path.join(\"vae_classifier_pairs\", \"classifier_{}_{}.pth\".format(en_drate, de_drate)))\n",
    "\n",
    "\n",
    "#save the vae_classifier_pairs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, vae_classifier_pair in enumerate(vae_classifier_pairs):\n",
    "    print(\" ebn_drate: \", vae_classifier_pair[\"en_drate\"], \"de_drate: \", vae_classifier_pair[\"de_drate\"])\n",
    "    vae_model = vae_classifier_pair[\"vae_model\"]\n",
    "    classifier = vae_classifier_pair[\"classifier\"]\n",
    "    acc = eval_vae_classifier(classifier, vae_model, val_loader, device)\n",
    "    print(\"Accuracy: \", acc)\n",
    "    plt.scatter(vae_classifier_pair[\"en_drate\"], vae_classifier_pair[\"de_drate\"], s=acc*1000, alpha=0.5)\n",
    "    \n",
    "plt.xlabel(\"en_drate\")\n",
    "plt.ylabel(\"de_drate\")\n",
    "plt.title(\"Accuracy of Classifier with Drop Label Rate\")\n",
    "plt.colorbar(label=\"Accuracy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebc1873",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make a table of en_drate and de_drate and the accuracy of the classifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, vae_classifier_pair in enumerate(vae_classifier_pairs):\n",
    "    print(\" ebn_drate: \", vae_classifier_pair[\"en_drate\"], \"de_drate: \", vae_classifier_pair[\"de_drate\"])\n",
    "    vae_model = vae_classifier_pair[\"vae_model\"]\n",
    "    classifier = vae_classifier_pair[\"classifier\"]\n",
    "    acc = eval_vae_classifier(classifier, vae_model, val_loader, device)\n",
    "    print(\"Accuracy: \", acc)\n",
    "    plt.scatter(vae_classifier_pair[\"en_drate\"], vae_classifier_pair[\"de_drate\"], s=acc*1000, alpha=0.5)\n",
    "    \n",
    "plt.xlabel(\"en_drate\")\n",
    "plt.ylabel(\"de_drate\")\n",
    "plt.title(\"Accuracy of Classifier with Drop Label Rate\")\n",
    "plt.colorbar(label=\"Accuracy\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d09779",
   "metadata": {},
   "source": [
    "## After Save, some experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff97f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 10 classes from val set, encode, and decode to another class\n",
    "cvae = CVAE(img_dim, label_dim, latent_dim, hidden_dim)\n",
    "cvae.load_state_dict(load_model('vae_classifier_pairs/vae_model_1.0_0.0.pth')[0])\n",
    "cvae.to(device)\n",
    "cvae.eval()\n",
    "\n",
    "num_samples_per_class = 10\n",
    "num_classes = 11\n",
    "# print(augmented_val_set.data.shape)\n",
    "# print(augmented_val_set.targets.shape)\n",
    "\n",
    "aug_data, aug_targets = augmented_val_set[0]\n",
    "print(aug_data.shape, aug_targets.shape)\n",
    "\n",
    "shuffled_idx = torch.randperm(len(augmented_val_set))\n",
    "# shuffled_data, shuffled_labels  = augmented_val_set[shuffled_idx]\n",
    "shuffled_data = []\n",
    "shuffled_labels = []\n",
    "for i in range(len(augmented_val_set)):\n",
    "    x, y = augmented_val_set[shuffled_idx[i]]\n",
    "    shuffled_data.append(x)\n",
    "    shuffled_labels.append(y)\n",
    "shuffled_data = torch.stack(shuffled_data)\n",
    "shuffled_labels = torch.stack(shuffled_labels)\n",
    "\n",
    "sample_per_class = {}\n",
    "for i in range(num_classes-1):\n",
    "    sample_per_class[i] = []\n",
    "\n",
    "for i in range(num_classes-1):\n",
    "    for j in range(len(shuffled_data)):\n",
    "        x, y = shuffled_data[j], shuffled_labels[j]\n",
    "        y = y[0]\n",
    "        # plt.imshow(x.cpu().numpy().squeeze(), cmap='gray')\n",
    "        # plt.axis('off')\n",
    "        # plt.title(f'Class {y} -> {i}')\n",
    "        # plt.show()\n",
    "        if y == i:\n",
    "            sample_per_class[i].append(x)\n",
    "            if len(sample_per_class[i]) == num_samples_per_class:\n",
    "                break\n",
    "    sample_per_class[i] = torch.stack(sample_per_class[i])\n",
    "    sample_per_class[i] = sample_per_class[i]\n",
    "\n",
    "# plot the samples\n",
    "# for i in range(num_classes):\n",
    "#     for j in range(num_samples_per_class):\n",
    "#         plt.subplot(num_classes, num_samples_per_class, i * num_samples_per_class + j + 1)\n",
    "#         plt.imshow(sample_per_class[i][j].cpu().numpy().squeeze(), cmap='gray')\n",
    "#         plt.axis('off')\n",
    "#         # plt.title(f'Class {i} -> {j}')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(num_classes-1):\n",
    "    sample_per_class[i] = sample_per_class[i].view(\n",
    "        num_samples_per_class, 1, 28, 28).to(device)\n",
    "    for j in range(num_classes):\n",
    "        label = torch.zeros(num_samples_per_class, num_classes).to(device)\n",
    "        label[:, j] = 1\n",
    "        ori_label = torch.zeros(num_samples_per_class, num_classes).to(device)\n",
    "        ori_label[:, i] = 1\n",
    "        zero_label = torch.zeros(num_samples_per_class, num_classes).to(device)\n",
    "        uniform_label = torch.ones(num_samples_per_class, num_classes).to(device)\n",
    "        uniform_label = uniform_label / num_classes\n",
    "\n",
    "        sample_per_class[i] = sample_per_class[i].to(device)\n",
    "        # print(ori_label.shape, label.shape)\n",
    "        samples = cvae.decode(\n",
    "            cvae.reparamaterize(*cvae.encode_param(sample_per_class[i], label)), label).detach()\n",
    "        imgs = samples.view(num_samples_per_class, 1, 28, 28).clamp(0., 1.)\n",
    "        torchvision.utils.save_image(imgs, os.path.join(\n",
    "            'vae/generated', f'{i}_{j}.png'), nrow=num_samples_per_class)\n",
    "        \n",
    "        plt.subplot(num_classes, num_classes, i * num_classes + j + 1)\n",
    "        plt.imshow(imgs[0].cpu().numpy().squeeze(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        # plt.title(f'Class {i} -> {j}')\n",
    "# plt.tight_layout()\n",
    "plt.savefig('vae/generated/vae.png')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c7512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 10 classes from val set, encode, and decode to another class\n",
    "from sklearn.manifold import TSNE\n",
    "cvae.load_state_dict(load_model('vae_classifier_pairs/vae_model_1.0_0.0.pth')[0])\n",
    "cvae.to(device)\n",
    "cvae.eval()\n",
    "\n",
    "num_samples_per_class = 500\n",
    "num_classes = 11\n",
    "\n",
    "aug_data, aug_targets = augmented_val_set[0]\n",
    "print(aug_data.shape, aug_targets.shape)\n",
    "\n",
    "shuffled_idx = torch.randperm(len(augmented_val_set))\n",
    "# shuffled_data, shuffled_labels  = augmented_val_set[shuffled_idx]\n",
    "shuffled_data = []\n",
    "shuffled_labels = []\n",
    "for i in range(len(augmented_val_set)):\n",
    "    x, y = augmented_val_set[shuffled_idx[i]]\n",
    "    shuffled_data.append(x)\n",
    "    shuffled_labels.append(y)\n",
    "shuffled_data = torch.stack(shuffled_data)\n",
    "shuffled_labels = torch.stack(shuffled_labels)\n",
    "\n",
    "sample_per_class = {}\n",
    "for i in range(num_classes):\n",
    "    sample_per_class[i] = []\n",
    "\n",
    "for i in range(num_classes-1):\n",
    "    for j in range(len(shuffled_data)):\n",
    "        x, y = shuffled_data[j], shuffled_labels[j]\n",
    "        y= y[0]\n",
    "        if y == i:\n",
    "            sample_per_class[i].append(x)\n",
    "            if len(sample_per_class[i]) == num_samples_per_class:\n",
    "                break\n",
    "    sample_per_class[i] = torch.stack(sample_per_class[i])\n",
    "    sample_per_class[i] = sample_per_class[i] / 255.0\n",
    "    # print(sample_per_class[i].shape)\n",
    "\n",
    "distrubutions = torch.zeros((num_classes, num_classes))\n",
    "\n",
    "\n",
    "latent_mu = []\n",
    "latent_labels = []\n",
    "for j in range(num_classes-1):\n",
    "    label = torch.zeros(num_samples_per_class, num_classes).to(device)\n",
    "    label[:, j] = 1\n",
    "    sample_per_class[j] = sample_per_class[j].to(device)\n",
    "    mu, logstd = cvae.encode_param(sample_per_class[j],label)\n",
    "    latent_mu.append(mu.detach().cpu().numpy())\n",
    "    latent_labels.extend([j] * num_samples_per_class)\n",
    "\n",
    "# 将所有 latent space 数据合并\n",
    "latent_mu = np.vstack(latent_mu)\n",
    "latent_labels = np.array(latent_labels)\n",
    "\n",
    "# 使用 t-SNE 降维\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "latent_2d = tsne.fit_transform(latent_mu)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(11, 11))\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, num_classes))\n",
    "for j in range(num_classes):\n",
    "    idx = latent_labels == j\n",
    "    plt.scatter(latent_2d[idx, 0], latent_2d[idx, 1], c=colors[j], label=f'Class {j}', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.title(f't-SNE Visualization of {i}\\'s Latent Space')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49558d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 10 classes from val set, encode, and decode to another class\n",
    "cvae = CVAE(img_dim, label_dim, latent_dim, hidden_dim)\n",
    "cvae.load_state_dict(load_model('vae_classifier_pairs/vae_model_1.0_0.0.pth')[0])\n",
    "cvae.to(device)\n",
    "cvae.eval()\n",
    "\n",
    "num_samples_per_class = 10\n",
    "num_classes = 11\n",
    "# print(augmented_val_set.data.shape)\n",
    "# print(augmented_val_set.targets.shape)\n",
    "\n",
    "\n",
    "for i in range(num_classes-1):\n",
    "    for j in range(num_classes):\n",
    "        label = torch.zeros(num_samples_per_class, num_classes).to(device)\n",
    "        label[:, j] = 1\n",
    "        ori_label = torch.zeros(num_samples_per_class, num_classes).to(device)\n",
    "        ori_label[:, i] = 1\n",
    "        zero_label = torch.zeros(num_samples_per_class, num_classes).to(device)\n",
    "        uniform_label = torch.ones(num_samples_per_class, num_classes).to(device)\n",
    "        uniform_label = uniform_label / num_classes\n",
    "\n",
    "        # print(ori_label.shape, label.shape)\n",
    "        samples = cvae.sample_images(\n",
    "            label, save=False)\n",
    "        imgs = samples.view(num_samples_per_class, 1, 28, 28).clamp(0., 1.)\n",
    "        torchvision.utils.save_image(imgs, os.path.join(\n",
    "            'vae/generated', f'{i}_{j}.png'), nrow=num_samples_per_class)\n",
    "        \n",
    "        plt.subplot(num_classes, num_classes, i * num_classes + j + 1)\n",
    "        plt.imshow(imgs[0].cpu().numpy().squeeze(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        # plt.title(f'Class {i} -> {j}')\n",
    "# plt.tight_layout()\n",
    "plt.savefig('vae/generated/vae.png')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
